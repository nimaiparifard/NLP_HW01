{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from gensim.models import Word2Vec\n",
    "from hazm import Normalizer, word_tokenize, stopwords_list, Stemmer, Lemmatizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T10:54:07.979112200Z",
     "start_time": "2024-04-17T10:54:07.961240300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-17T10:54:13.913557500Z",
     "start_time": "2024-04-17T10:54:08.705913300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "val = pd.read_csv(\"val.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "# Create a normalizer object\n",
    "normalizer = Normalizer()\n",
    "def remove_u200c(text):\n",
    "    return text.replace('\\u200c', '')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Normalize the text\n",
    "    text = normalizer.normalize(text)\n",
    "    text = normalizer.remove_specials_chars(text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    words = [remove_u200c(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "train['content'] = train['content'].apply(preprocess_text)\n",
    "val['content'] = val['content'].apply(preprocess_text)\n",
    "test['content'] = test['content'].apply(preprocess_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T10:55:26.900432Z",
     "start_time": "2024-04-17T10:54:13.904581800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "# Create a stemmer object\n",
    "stemmer = Stemmer()\n",
    "\n",
    "# Create a lemmatizer object\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "# Get the list of Persian stopwords\n",
    "stopwords = stopwords_list()\n",
    "\n",
    "def preprocess_text(words):\n",
    "    # Remove stopwords, apply stemming and lemmatization\n",
    "    words = [lemmatizer.lemmatize(stemmer.stem(word)) for word in words if word not in stopwords]\n",
    "    return words\n",
    "\n",
    "train['content'] = train['content'].apply(preprocess_text)\n",
    "test['content'] = test['content'].apply(preprocess_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T10:56:38.078180500Z",
     "start_time": "2024-04-17T10:55:26.906824400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    " # Train Word2Vec model\n",
    "model = Word2Vec(train['content'], min_count=1, sg=1, vector_size=200)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T10:57:53.631840100Z",
     "start_time": "2024-04-17T10:56:38.081177500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_avg_vectors = train['content'].apply(lambda words: np.mean([model.wv[word] for word in words], axis=0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T17:58:19.938534600Z",
     "start_time": "2024-04-11T17:58:11.503785Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def check_existence(word, model, vector_size):\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except:\n",
    "        return np.zeros(vector_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T17:58:19.948576300Z",
     "start_time": "2024-04-11T17:58:19.944519900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "test_avg_vectors = test['content'].apply(lambda words: np.mean([check_existence(word, model, 200) for word in words], axis=0))\n",
    "val_avg_vectors = val['content'].apply(lambda words: np.mean([check_existence(word, model, 200) for word in words], axis=0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T17:58:26.800815400Z",
     "start_time": "2024-04-11T17:58:19.954637Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'n_neighbors': 11}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.72      0.77       217\n",
      "           1       0.84      0.76      0.80       156\n",
      "           2       0.91      0.87      0.89       197\n",
      "           3       0.74      0.84      0.79       227\n",
      "           4       0.88      0.91      0.89       244\n",
      "           5       0.90      0.91      0.91       256\n",
      "           6       0.99      0.94      0.97       138\n",
      "           7       0.81      0.86      0.83       209\n",
      "\n",
      "    accuracy                           0.85      1644\n",
      "   macro avg       0.86      0.85      0.86      1644\n",
      "weighted avg       0.86      0.85      0.85      1644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for the KNN classifier\n",
    "param_grid = {'n_neighbors': [3, 5, 7, 9, 11]}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5,n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(list(train_avg_vectors), train['label'])\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "\n",
    "# Use the best estimator to make predictions on the test data\n",
    "best_knn = grid_search.best_estimator_\n",
    "predictions = best_knn.predict(list(test_avg_vectors))\n",
    "\n",
    "# Print the classification report for the test data predictions\n",
    "print(classification_report(test['label'], predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T17:58:57.132343700Z",
     "start_time": "2024-04-11T17:58:26.817769Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 30\u001B[0m\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m word2weight\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# Apply TF-IDF to the content\u001B[39;00m\n\u001B[1;32m---> 30\u001B[0m word2weight \u001B[38;5;241m=\u001B[39m tf_idf(train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "Cell \u001B[1;32mIn[16], line 22\u001B[0m, in \u001B[0;36mtf_idf\u001B[1;34m(docs)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtf_idf\u001B[39m(docs):\n\u001B[0;32m     21\u001B[0m     word2weight \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m---> 22\u001B[0m     idf \u001B[38;5;241m=\u001B[39m inverse_document_frequency(docs)\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m docs:\n\u001B[0;32m     24\u001B[0m         tf \u001B[38;5;241m=\u001B[39m term_frequency(doc)\n",
      "Cell \u001B[1;32mIn[16], line 16\u001B[0m, in \u001B[0;36minverse_document_frequency\u001B[1;34m(docs)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m all_words:\n\u001B[0;32m     15\u001B[0m     contains_word \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m doc: word \u001B[38;5;129;01min\u001B[39;00m doc, docs)\n\u001B[1;32m---> 16\u001B[0m     idf[word] \u001B[38;5;241m=\u001B[39m log(\u001B[38;5;28mlen\u001B[39m(docs)\u001B[38;5;241m/\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28msum\u001B[39m(contains_word)))\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m idf\n",
      "Cell \u001B[1;32mIn[16], line 15\u001B[0m, in \u001B[0;36minverse_document_frequency.<locals>.<lambda>\u001B[1;34m(doc)\u001B[0m\n\u001B[0;32m     13\u001B[0m all_words \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(word \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m docs \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m doc)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m all_words:\n\u001B[1;32m---> 15\u001B[0m     contains_word \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m doc: word \u001B[38;5;129;01min\u001B[39;00m doc, docs)\n\u001B[0;32m     16\u001B[0m     idf[word] \u001B[38;5;241m=\u001B[39m log(\u001B[38;5;28mlen\u001B[39m(docs)\u001B[38;5;241m/\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28msum\u001B[39m(contains_word)))\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m idf\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Calculate term frequency\n",
    "def term_frequency(doc):\n",
    "    counts = Counter(doc)\n",
    "    return {word: count/len(doc) for word, count in counts.items()}\n",
    "\n",
    "# Calculate inverse document frequency\n",
    "def inverse_document_frequency(docs):\n",
    "    idf = {}\n",
    "    all_words = set(word for doc in docs for word in doc)\n",
    "    for word in all_words:\n",
    "        contains_word = map(lambda doc: word in doc, docs)\n",
    "        idf[word] = log(len(docs)/(1 + sum(contains_word)))\n",
    "    return idf\n",
    "\n",
    "# Calculate TF-IDF\n",
    "def tf_idf(docs):\n",
    "    word2weight = {}\n",
    "    idf = inverse_document_frequency(docs)\n",
    "    for doc in docs:\n",
    "        tf = term_frequency(doc)\n",
    "        for word, freq in tf.items():\n",
    "            word2weight[word] = freq * idf[word]\n",
    "    return word2weight\n",
    "\n",
    "# Apply TF-IDF to the content\n",
    "word2weight = tf_idf(train['content'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T18:18:11.270395900Z",
     "start_time": "2024-04-11T18:03:20.572188400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MY IMPLEMENTATION of tf idf need too much time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "def compute_tfidf(dataframe):\n",
    "    dataframe_copy = dataframe.copy()\n",
    "    dataframe_copy['content'] = dataframe_copy['content'].astype(str)\n",
    "\n",
    "    # Initialize the TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(dataframe_copy['content'])\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "    return tfidf_df\n",
    "\n",
    "\n",
    "tfidf_dataframe_train = compute_tfidf(train)\n",
    "tfidf_dataframe_test = compute_tfidf(test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T10:59:09.731753500Z",
     "start_time": "2024-04-17T10:58:56.584376800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "data": {
      "text/plain": "list"
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train['content'][1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T10:59:48.853706Z",
     "start_time": "2024-04-17T10:59:48.822367900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "def weighted_average_vector(words, tfidf_df, model, vector_size, content_index):\n",
    "    weights = [tfidf_df.at[content_index, word] if word in tfidf_df.columns else 0 for word in words]\n",
    "    vectors = [check_existence(word[2:-1], model, vector_size) * weight for word, weight in zip(words, weights)]\n",
    "    weighted_vector = np.sum(vectors, axis=0) / sum(weights) if sum(weights) > 0 else np.zeros(vector_size)\n",
    "    return weighted_vector\n",
    "\n",
    "averaged_tfidf_train_vector = pd.DataFrame()\n",
    "averaged_tfidf_train_vector['weighted_vectors'] = train.apply(lambda row: weighted_average_vector(row['content'], tfidf_dataframe_train, model, 200, row.name), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T11:06:49.876375500Z",
     "start_time": "2024-04-17T10:59:59.901974200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "averaged_tfidf_test_vector = pd.DataFrame()\n",
    "averaged_tfidf_test_vector['weighted_vectors'] = test.apply(lambda row: weighted_average_vector(row['content'], tfidf_dataframe_test, model, 200, row.name), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T11:10:10.610202Z",
     "start_time": "2024-04-17T11:09:31.507482800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "averaged_tfidf_train_vector = pd.DataFrame(averaged_tfidf_train_vector['weighted_vectors'].tolist())\n",
    "averaged_tfidf_test_vector = pd.DataFrame(averaged_tfidf_test_vector['weighted_vectors'].tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T11:17:16.115252400Z",
     "start_time": "2024-04-17T11:17:15.081444300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "data": {
      "text/plain": "            0         1         2         3         4         5         6    \\\n0      0.035862 -0.112808  0.114000  0.088515 -0.078202  0.024686 -0.053142   \n1     -0.025810 -0.017736  0.053696  0.014416 -0.054654 -0.021744 -0.024179   \n2     -0.024027 -0.122084  0.055143  0.028036 -0.060900  0.005088 -0.015501   \n3     -0.002543 -0.018871  0.011171  0.005699 -0.021981  0.003959 -0.001240   \n4      0.016554 -0.048106  0.045708  0.022023 -0.041868  0.014961 -0.004186   \n...         ...       ...       ...       ...       ...       ...       ...   \n13309 -0.026748 -0.077005  0.038115  0.047939 -0.068217 -0.033169 -0.011428   \n13310 -0.011747 -0.209270  0.100183  0.047660 -0.096377  0.061074 -0.055249   \n13311 -0.004446 -0.001938  0.000160  0.034167 -0.064746 -0.022242 -0.007561   \n13312  0.030782  0.027556  0.032795  0.012134 -0.067947  0.000592 -0.019363   \n13313  0.029903 -0.356423  0.195630  0.135615 -0.111842  0.136100 -0.110145   \n\n            7         8         9    ...       190       191       192  \\\n0      0.164881  0.021225 -0.023193  ...  0.093822  0.045869 -0.092711   \n1      0.078300  0.004737  0.025706  ...  0.091946  0.016544 -0.050197   \n2      0.177120  0.054914  0.030419  ...  0.033569 -0.001953 -0.090035   \n3      0.056898  0.001188  0.012098  ...  0.030257  0.008301 -0.030636   \n4      0.099906  0.013650  0.006763  ...  0.054721  0.002409 -0.040711   \n...         ...       ...       ...  ...       ...       ...       ...   \n13309  0.104179 -0.006156  0.005152  ...  0.078293  0.018336 -0.080581   \n13310  0.084722  0.023599 -0.000888  ...  0.010384  0.032084 -0.085715   \n13311  0.201843  0.023876  0.010571  ...  0.076916  0.025338 -0.112461   \n13312  0.135255  0.034717  0.000056  ...  0.099718  0.020959 -0.068113   \n13313  0.154503  0.080141 -0.007940  ...  0.071683  0.021458 -0.171982   \n\n            193       194       195       196       197       198       199  \n0     -0.035211  0.082029  0.180364 -0.094961 -0.223556  0.008804  0.057009  \n1     -0.074023  0.059599  0.089849 -0.007602 -0.063566  0.019655  0.000181  \n2     -0.038973  0.060554  0.159910 -0.020167 -0.234681 -0.011592  0.017877  \n3     -0.006380  0.023158  0.047930  0.002099 -0.064154  0.008579 -0.009091  \n4     -0.024341  0.050985  0.055208 -0.025623 -0.085176  0.013984 -0.011667  \n...         ...       ...       ...       ...       ...       ...       ...  \n13309 -0.036076  0.044301  0.088154 -0.030073 -0.142752  0.026356  0.030831  \n13310  0.016863 -0.016578  0.158788 -0.075489 -0.103788  0.009507 -0.005941  \n13311 -0.097209  0.141591  0.100885  0.009437 -0.214768 -0.009785 -0.019890  \n13312 -0.045146  0.080847  0.137774 -0.011096 -0.130354 -0.003231 -0.043431  \n13313  0.012966 -0.051509  0.164049 -0.138380 -0.165606  0.067222 -0.017461  \n\n[13314 rows x 200 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>190</th>\n      <th>191</th>\n      <th>192</th>\n      <th>193</th>\n      <th>194</th>\n      <th>195</th>\n      <th>196</th>\n      <th>197</th>\n      <th>198</th>\n      <th>199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.035862</td>\n      <td>-0.112808</td>\n      <td>0.114000</td>\n      <td>0.088515</td>\n      <td>-0.078202</td>\n      <td>0.024686</td>\n      <td>-0.053142</td>\n      <td>0.164881</td>\n      <td>0.021225</td>\n      <td>-0.023193</td>\n      <td>...</td>\n      <td>0.093822</td>\n      <td>0.045869</td>\n      <td>-0.092711</td>\n      <td>-0.035211</td>\n      <td>0.082029</td>\n      <td>0.180364</td>\n      <td>-0.094961</td>\n      <td>-0.223556</td>\n      <td>0.008804</td>\n      <td>0.057009</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.025810</td>\n      <td>-0.017736</td>\n      <td>0.053696</td>\n      <td>0.014416</td>\n      <td>-0.054654</td>\n      <td>-0.021744</td>\n      <td>-0.024179</td>\n      <td>0.078300</td>\n      <td>0.004737</td>\n      <td>0.025706</td>\n      <td>...</td>\n      <td>0.091946</td>\n      <td>0.016544</td>\n      <td>-0.050197</td>\n      <td>-0.074023</td>\n      <td>0.059599</td>\n      <td>0.089849</td>\n      <td>-0.007602</td>\n      <td>-0.063566</td>\n      <td>0.019655</td>\n      <td>0.000181</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.024027</td>\n      <td>-0.122084</td>\n      <td>0.055143</td>\n      <td>0.028036</td>\n      <td>-0.060900</td>\n      <td>0.005088</td>\n      <td>-0.015501</td>\n      <td>0.177120</td>\n      <td>0.054914</td>\n      <td>0.030419</td>\n      <td>...</td>\n      <td>0.033569</td>\n      <td>-0.001953</td>\n      <td>-0.090035</td>\n      <td>-0.038973</td>\n      <td>0.060554</td>\n      <td>0.159910</td>\n      <td>-0.020167</td>\n      <td>-0.234681</td>\n      <td>-0.011592</td>\n      <td>0.017877</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.002543</td>\n      <td>-0.018871</td>\n      <td>0.011171</td>\n      <td>0.005699</td>\n      <td>-0.021981</td>\n      <td>0.003959</td>\n      <td>-0.001240</td>\n      <td>0.056898</td>\n      <td>0.001188</td>\n      <td>0.012098</td>\n      <td>...</td>\n      <td>0.030257</td>\n      <td>0.008301</td>\n      <td>-0.030636</td>\n      <td>-0.006380</td>\n      <td>0.023158</td>\n      <td>0.047930</td>\n      <td>0.002099</td>\n      <td>-0.064154</td>\n      <td>0.008579</td>\n      <td>-0.009091</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.016554</td>\n      <td>-0.048106</td>\n      <td>0.045708</td>\n      <td>0.022023</td>\n      <td>-0.041868</td>\n      <td>0.014961</td>\n      <td>-0.004186</td>\n      <td>0.099906</td>\n      <td>0.013650</td>\n      <td>0.006763</td>\n      <td>...</td>\n      <td>0.054721</td>\n      <td>0.002409</td>\n      <td>-0.040711</td>\n      <td>-0.024341</td>\n      <td>0.050985</td>\n      <td>0.055208</td>\n      <td>-0.025623</td>\n      <td>-0.085176</td>\n      <td>0.013984</td>\n      <td>-0.011667</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13309</th>\n      <td>-0.026748</td>\n      <td>-0.077005</td>\n      <td>0.038115</td>\n      <td>0.047939</td>\n      <td>-0.068217</td>\n      <td>-0.033169</td>\n      <td>-0.011428</td>\n      <td>0.104179</td>\n      <td>-0.006156</td>\n      <td>0.005152</td>\n      <td>...</td>\n      <td>0.078293</td>\n      <td>0.018336</td>\n      <td>-0.080581</td>\n      <td>-0.036076</td>\n      <td>0.044301</td>\n      <td>0.088154</td>\n      <td>-0.030073</td>\n      <td>-0.142752</td>\n      <td>0.026356</td>\n      <td>0.030831</td>\n    </tr>\n    <tr>\n      <th>13310</th>\n      <td>-0.011747</td>\n      <td>-0.209270</td>\n      <td>0.100183</td>\n      <td>0.047660</td>\n      <td>-0.096377</td>\n      <td>0.061074</td>\n      <td>-0.055249</td>\n      <td>0.084722</td>\n      <td>0.023599</td>\n      <td>-0.000888</td>\n      <td>...</td>\n      <td>0.010384</td>\n      <td>0.032084</td>\n      <td>-0.085715</td>\n      <td>0.016863</td>\n      <td>-0.016578</td>\n      <td>0.158788</td>\n      <td>-0.075489</td>\n      <td>-0.103788</td>\n      <td>0.009507</td>\n      <td>-0.005941</td>\n    </tr>\n    <tr>\n      <th>13311</th>\n      <td>-0.004446</td>\n      <td>-0.001938</td>\n      <td>0.000160</td>\n      <td>0.034167</td>\n      <td>-0.064746</td>\n      <td>-0.022242</td>\n      <td>-0.007561</td>\n      <td>0.201843</td>\n      <td>0.023876</td>\n      <td>0.010571</td>\n      <td>...</td>\n      <td>0.076916</td>\n      <td>0.025338</td>\n      <td>-0.112461</td>\n      <td>-0.097209</td>\n      <td>0.141591</td>\n      <td>0.100885</td>\n      <td>0.009437</td>\n      <td>-0.214768</td>\n      <td>-0.009785</td>\n      <td>-0.019890</td>\n    </tr>\n    <tr>\n      <th>13312</th>\n      <td>0.030782</td>\n      <td>0.027556</td>\n      <td>0.032795</td>\n      <td>0.012134</td>\n      <td>-0.067947</td>\n      <td>0.000592</td>\n      <td>-0.019363</td>\n      <td>0.135255</td>\n      <td>0.034717</td>\n      <td>0.000056</td>\n      <td>...</td>\n      <td>0.099718</td>\n      <td>0.020959</td>\n      <td>-0.068113</td>\n      <td>-0.045146</td>\n      <td>0.080847</td>\n      <td>0.137774</td>\n      <td>-0.011096</td>\n      <td>-0.130354</td>\n      <td>-0.003231</td>\n      <td>-0.043431</td>\n    </tr>\n    <tr>\n      <th>13313</th>\n      <td>0.029903</td>\n      <td>-0.356423</td>\n      <td>0.195630</td>\n      <td>0.135615</td>\n      <td>-0.111842</td>\n      <td>0.136100</td>\n      <td>-0.110145</td>\n      <td>0.154503</td>\n      <td>0.080141</td>\n      <td>-0.007940</td>\n      <td>...</td>\n      <td>0.071683</td>\n      <td>0.021458</td>\n      <td>-0.171982</td>\n      <td>0.012966</td>\n      <td>-0.051509</td>\n      <td>0.164049</td>\n      <td>-0.138380</td>\n      <td>-0.165606</td>\n      <td>0.067222</td>\n      <td>-0.017461</td>\n    </tr>\n  </tbody>\n</table>\n<p>13314 rows Ã— 200 columns</p>\n</div>"
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averaged_tfidf_train_vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T11:19:05.169006300Z",
     "start_time": "2024-04-17T11:19:05.110469500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for KNN:  {'n_neighbors': 20}\n",
      "Best cross-validation score for KNN: 0.343\n",
      "Best parameters for RandomForest:  {'max_features': 'sqrt', 'n_estimators': 200}\n",
      "Best cross-validation score for RandomForest: 0.401\n",
      "Best parameters for SVM:  {'C': 10, 'kernel': 'rbf'}\n",
      "Best cross-validation score for SVM: 0.504\n",
      "Best classifier is SVM with a score of 0.504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.46      0.45       217\n",
      "           1       0.48      0.42      0.45       156\n",
      "           2       0.54      0.53      0.54       197\n",
      "           3       0.45      0.50      0.47       227\n",
      "           4       0.53      0.53      0.53       244\n",
      "           5       0.55      0.62      0.58       256\n",
      "           6       0.71      0.59      0.65       138\n",
      "           7       0.54      0.50      0.52       209\n",
      "\n",
      "    accuracy                           0.52      1644\n",
      "   macro avg       0.53      0.52      0.52      1644\n",
      "weighted avg       0.52      0.52      0.52      1644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grids = {\n",
    "    'KNN': {'n_neighbors': [3, 5, 7, 9, 11, 20]},\n",
    "    'RandomForest': {'n_estimators': [50, 100, 200], 'max_features': ['sqrt', 'log2']},\n",
    "    'SVM': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "}\n",
    "\n",
    "classifiers = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'SVM': SVC()\n",
    "}\n",
    "\n",
    "best_scores = {}\n",
    "best_params = {}\n",
    "best_estimators = {}\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    grid_search = GridSearchCV(classifier, param_grids[name], cv=5, n_jobs=-1)\n",
    "    grid_search.fit(averaged_tfidf_train_vector, train['label'])\n",
    "    best_scores[name] = grid_search.best_score_\n",
    "    best_params[name] = grid_search.best_params_\n",
    "    best_estimators[name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {name}: \", grid_search.best_params_)\n",
    "    print(f\"Best cross-validation score for {name}: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "best_classifier_name = max(best_scores, key=best_scores.get)\n",
    "print(f\"Best classifier is {best_classifier_name} with a score of {best_scores[best_classifier_name]:.3f}\")\n",
    "best_classifier = best_estimators[best_classifier_name]\n",
    "predictions = best_classifier.predict(averaged_tfidf_test_vector)\n",
    "print(classification_report(test['label'], predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T11:41:56.105792200Z",
     "start_time": "2024-04-17T11:27:02.492411300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of wighted vector with knn 0.5206812652068127\n",
      "f1 score of wighted vector with knn 0.5233737501517393\n"
     ]
    }
   ],
   "source": [
    "# P4 implementation of multiclass Accuracy and F1 score\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct = np.sum(y_true == y_pred)\n",
    "    total = len(y_true)\n",
    "    return correct / total\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    # Calculate precision and recall for each class\n",
    "    classes = np.unique(y_true)\n",
    "    f1_scores = []\n",
    "    for cls in classes:\n",
    "        tp = np.sum((y_true == cls) & (y_pred == cls))\n",
    "        fp = np.sum((y_true != cls) & (y_pred == cls))\n",
    "        fn = np.sum((y_true == cls) & (y_pred != cls))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1)\n",
    "    # Calculate the average F1-score\n",
    "    return np.mean(f1_scores)\n",
    "print(f\"Accuracy of wighted vector with knn {accuracy(test['label'], predictions)}\")\n",
    "print(f\"f1 score of wighted vector with knn {f1_score(test['label'], predictions)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T16:09:26.339832200Z",
     "start_time": "2024-04-17T16:09:25.892221800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
